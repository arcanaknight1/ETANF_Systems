# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:10))
# aic list
step.forward.aic <- rep(0,10)
step.backward.aic <- rep(0,10)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:10){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
#running best subset to compare best choice
regfit.full <- regsubsets(ETANF~., data = etanf.x)
reg.summary <- summary(regfit.full)
reg.summary$cp
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
# rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=50))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:10))
# aic list
step.forward.aic <- rep(0,10)
step.backward.aic <- rep(0,10)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:10){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
#running best subset to compare best choice
#regfit.full <- regsubsets(ETANF~., data = etanf.x)
#reg.summary <- summary(regfit.full)
#reg.summary$cp
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
# rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=50))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:10))
# aic list
step.forward.aic <- rep(0,10)
step.backward.aic <- rep(0,10)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:10){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
fwd.model
#running best subset to compare best choice
#regfit.full <- regsubsets(ETANF~., data = etanf.x)
#reg.summary <- summary(regfit.full)
#reg.summary$cp
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
# rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=50))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:8))
# aic list
step.forward.aic <- rep(0,8)
step.backward.aic <- rep(0,8)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:8){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
fwd.model
#running best subset to compare best choice
#regfit.full <- regsubsets(ETANF~., data = etanf.x)
#reg.summary <- summary(regfit.full)
#reg.summary$cp
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
# rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=50))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:8))
# aic list
step.forward.aic <- rep(0,8)
step.backward.aic <- rep(0,8)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:8){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
fwd.model
#running best subset to compare best choice
regfit.full <- regsubsets(ETANF~., data = etanf.x)
reg.summary <- summary(regfit.full)
reg.summary$cp
reg.summary
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
# rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=50))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:8))
# aic list
step.forward.aic <- rep(0,8)
step.backward.aic <- rep(0,8)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:8){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
fwd.model
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
# rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=50))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:8))
# aic list
step.forward.aic <- rep(0,8)
step.backward.aic <- rep(0,8)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:8){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
fwd.model
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
# rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=50))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:8))
# aic list
step.forward.aic <- rep(0,8)
step.backward.aic <- rep(0,8)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:8){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
bwd.model <- bwd.model <- step(full, direction = "backward", trace = FALSE)
bwd.model
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=50))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:8))
# aic list
step.forward.aic <- rep(0,8)
step.backward.aic <- rep(0,8)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:8){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
bwd.model <- bwd.model <- step(full, direction = "backward", trace = FALSE)
bwd.model
rf1
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=20))
# # test for variable importance
# set.seed(1234)
# rf.test1 <- varimp(rf1)
# rf.test2 <- varimp(rf1, conditional = T)
# rf.test1
# rf.test2
# AUC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:8))
# aic list
step.forward.aic <- rep(0,8)
step.backward.aic <- rep(0,8)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:8){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for forward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
bwd.model <- bwd.model <- step(full, direction = "backward", trace = FALSE)
bwd.model
rf1
AIC(rf1)
### Variable selection
#subsetting data to remove extra location descriptors & missing records from dependent & Independent variables for variable selection
etanf.x <- na.omit(ETANFdata[, c(3,5:18)])
# fit the random forest
# rf1 <- cforest(ETANF ~ ., data= etanf.x, control=cforest_unbiased(mtry=2,ntree=20))
# AIC by crossvalidation for each variable selection method
testCV <- function(DATA){
# combine two columns
func.data <- data.frame(DATA)
# Count the number of row of entire dataframe
n.data <- nrow(func.data)
# divide the number of rows in the dataframe into ten groups
folds <- split(sample(n.data, n.data ,replace=FALSE), as.factor(1:8))
# aic list
step.forward.aic <- rep(0,8)
step.backward.aic <- rep(0,8)
result<- data.frame("method" = character(), "aic" = numeric(), stringsAsFactors = FALSE)
for (j in 1:8){
test.data <- func.data[folds[[j]], ]
n.test <-nrow(test.data)
train.data <- func.data[-folds[[j]], ]
#forward stepwise model
null = glm(ETANF~1, data = na.omit(train.data), family = "poisson")
full = glm(ETANF~., data = na.omit(train.data), family = "poisson")
fwd.model <- step(null, scope = list(lower = null, upper = full), direction = "forward", trace = FALSE)
step.forward.aic[j] <- AIC(fwd.model)
#backward stepwise model
bwd.model <- step(full, direction = "backward", trace = FALSE)
step.backward.aic [j] <- AIC(bwd.model)
}
result <- rbind(result, data.frame(method = "forward", auc = mean(step.forward.aic)))
result <- rbind(result, data.frame(method = "backward", auc = mean(step.backward.aic)))
return(result)
}
testCV(etanf.x)
#Counting predictors for backward selection
null = glm(ETANF~1, data = etanf.x, family = "poisson")
full = glm(ETANF~., data = etanf.x, family = "poisson")
bwd.model <- bwd.model <- step(full, direction = "backward", trace = FALSE)
bwd.model
